{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Yolo - Keras - Tensorflow backend\n",
    "\n",
    "YOLO paper: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Input, MaxPooling2D, BatchNormalization, Reshape, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from preprocessing import parse_annotation, BatchGenerator\n",
    "from yolo_utils import preprocess_image, decode_netout\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import wget\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tiny-yolo-voc-labels.txt', 'r') as f:\n",
    "    LABELS = [label.rstrip('\\n') for label in f.readlines()]\n",
    "\n",
    "ANCHORS = [1.08,1.19, 3.42,4.41, 6.63,11.38, 9.42,5.11, 16.62,10.52]\n",
    "\n",
    "IMAGE_H =  416\n",
    "IMAGE_W =  416\n",
    "CHANNELS = 3\n",
    "\n",
    "GRID_H,  GRID_W  = 13 , 13\n",
    "BOX = 5\n",
    "CLASS = len(LABELS)\n",
    "\n",
    "TRUE_BOX_BUFFER  = 50\n",
    "BATCH_SIZE       = 48\n",
    "\n",
    "EPSILON = 1e-8\n",
    "\n",
    "CLASS_WEIGHTS = np.ones(CLASS, dtype='float32')\n",
    "\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "WARM_UP_BATCHES  = 0\n",
    "\n",
    "DOWNLOAD_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, CHANNELS))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "'''\n",
    "https://keras.io/layers/convolutional/\n",
    "\n",
    "Conv2D(filters, kernel_size, strides=(1, 1), padding='valid',      \\\n",
    "     data_format=None, dilation_rate=(1, 1), activation=None,      \\\n",
    "     use_bias=True, kernel_initializer='glorot_uniform',           \\\n",
    "     bias_initializer='zeros', kernel_regularizer=None,            \\\n",
    "     bias_regularizer=None, activity_regularizer=None,             \\\n",
    "     kernel_constraint=None, bias_constraint=None)\n",
    "     \n",
    "https://keras.io/layers/normalization/#batchnormalization\n",
    "\n",
    "BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,          \\\n",
    "    center=True, scale=True, beta_initializer='zeros',             \\\n",
    "    gamma_initializer='ones', moving_mean_initializer='zeros',     \\\n",
    "    moving_variance_initializer='ones', beta_regularizer=None,     \\\n",
    "    gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "https://keras.io/layers/advanced-activations/#leakyrelu\n",
    "\n",
    "LeakyReLU(alpha=0.3) \n",
    "It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0, f(x) = x for x >= 0\n",
    "\n",
    "https://keras.io/layers/pooling/#maxpooling2d\n",
    "\n",
    "MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "\n",
    "https://keras.io/layers/core/    -- Reshape\n",
    "\n",
    "Reshape(target_shape)\n",
    "'''\n",
    "# Layer 1\n",
    "X = Conv2D(filters=16, kernel_size=(3,3), padding='same', use_bias=False, name='conv_1')(input_image)\n",
    "X = BatchNormalization(name='norm_1')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "# Layer 2\n",
    "X = Conv2D(filters=32, kernel_size=(3,3), padding='same', use_bias=False, name='conv_2')(X)\n",
    "X = BatchNormalization(name='norm_2')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "# Layer 3\n",
    "X = Conv2D(filters=64, kernel_size=(3,3), padding='same', use_bias=False, name='conv_3')(X)\n",
    "X = BatchNormalization(name='norm_3')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "# Layer 4\n",
    "X = Conv2D(filters=128, kernel_size=(3,3), padding='same', use_bias=False, name='conv_4')(X)\n",
    "X = BatchNormalization(name='norm_4')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "# Layer 5\n",
    "X = Conv2D(filters=256, kernel_size=(3,3), padding='same', use_bias=False, name='conv_5')(X)\n",
    "X = BatchNormalization(name='norm_5')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "X = MaxPooling2D()(X)\n",
    "\n",
    "# Layer 6\n",
    "X = Conv2D(filters=512, kernel_size=(3,3), padding='same', use_bias=False, name='conv_6')(X)\n",
    "X = BatchNormalization(name='norm_6')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "X = MaxPooling2D(strides=(1,1), padding='same')(X)\n",
    "\n",
    "# Layer 7\n",
    "X = Conv2D(filters=1024, kernel_size=(3,3), padding='same', use_bias=False, name='conv_7')(X)\n",
    "X = BatchNormalization(name='norm_7')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "\n",
    "# Layer 8\n",
    "X = Conv2D(filters=1024, kernel_size=(3,3), padding='same', use_bias=False, name='conv_8')(X)\n",
    "X = BatchNormalization(name='norm_8')(X)\n",
    "X = LeakyReLU(alpha=0.1)(X)\n",
    "\n",
    "# Layer 9\n",
    "# BOX=5, CLASS=20, GRID_H=13, GRID_W=13\n",
    "X = Conv2D(BOX * (4 + 1 + CLASS), kernel_size=(1, 1), kernel_initializer='he_normal')(X)\n",
    "Y = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(X)       # X = [None, 13, 13, 125], Y = [None, 13, 13, 5, 25]\n",
    "\n",
    "# small hack to allow true_boxes to be registered when Keras build the model \n",
    "# for more information: https://github.com/fchollet/keras/issues/2790\n",
    "#Y = Lambda(lambda args: args[0])([Y, true_boxes])\n",
    "\n",
    "# Create model\n",
    "model = Model([input_image, true_boxes], Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 416, 416, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 416, 416, 16)      432       \n",
      "_________________________________________________________________\n",
      "norm_1 (BatchNormalization)  (None, 416, 416, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 416, 416, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 208, 208, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 208, 208, 32)      4608      \n",
      "_________________________________________________________________\n",
      "norm_2 (BatchNormalization)  (None, 208, 208, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 208, 208, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 104, 104, 64)      18432     \n",
      "_________________________________________________________________\n",
      "norm_3 (BatchNormalization)  (None, 104, 104, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 52, 52, 128)       73728     \n",
      "_________________________________________________________________\n",
      "norm_4 (BatchNormalization)  (None, 52, 52, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 26, 26, 256)       294912    \n",
      "_________________________________________________________________\n",
      "norm_5 (BatchNormalization)  (None, 26, 26, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_6 (Conv2D)              (None, 13, 13, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "norm_6 (BatchNormalization)  (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_7 (Conv2D)              (None, 13, 13, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "norm_7 (BatchNormalization)  (None, 13, 13, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 13, 13, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv_8 (Conv2D)              (None, 13, 13, 1024)      9437184   \n",
      "_________________________________________________________________\n",
      "norm_8 (BatchNormalization)  (None, 13, 13, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 13, 13, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 125)       128125    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 13, 13, 5, 25)     0         \n",
      "=================================================================\n",
      "Total params: 15,867,885\n",
      "Trainable params: 15,861,773\n",
      "Non-trainable params: 6,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update network with pre-trained weights (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "if(os.path.isdir(\"weights\")):\n",
    "    if(os.path.exists(\"weights/yolov2-tiny-voc.weights\")):\n",
    "        print(\"file already exists\")\n",
    "    else:\n",
    "        os.chdir(\"weights/\")\n",
    "        url = 'https://pjreddie.com/media/files/yolov2-tiny-voc.weights'\n",
    "        wget.download(url)\n",
    "        os.chdir(\"../\")\n",
    "else:\n",
    "    os.makedirs(\"weights\")\n",
    "    os.chdir(\"weights/\")\n",
    "    url = 'https://pjreddie.com/media/files/yolov2-tiny-voc.weights'\n",
    "    wget.download(url)\n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight_reader:\n",
    "    def __init__(self, weight_file):\n",
    "        self.offset = 4\n",
    "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        read = self.all_weights[self.offset : self.offset + size]\n",
    "        self.offset = self.offset + size\n",
    "        return read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists('weights/yolov2-tiny-voc.weights')):\n",
    "    wr = weight_reader('weights/yolov2-tiny-voc.weights')\n",
    "    conv_ = 8\n",
    "    \n",
    "    for i in range(1, conv_ + 1):\n",
    "        conv_layer = model.get_layer('conv_' + str(i))\n",
    "    \n",
    "        if i < conv_:\n",
    "            norm_layer = model.get_layer('norm_' + str(i))\n",
    "            size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "        \n",
    "            beta  = wr.read_bytes(size)\n",
    "            gamma = wr.read_bytes(size)\n",
    "            mean  = wr.read_bytes(size)\n",
    "            var   = wr.read_bytes(size)\n",
    "        \n",
    "            weights = norm_layer.set_weights([gamma, beta, mean, var])\n",
    "        \n",
    "        if len(conv_layer.get_weights()) > 1:\n",
    "            bias   = wr.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "            kernel = wr.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "            kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "            kernel = kernel.transpose([2,3,1,0])\n",
    "            conv_layer.set_weights([kernel, bias])\n",
    "        else:\n",
    "            kernel = wr.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "            kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "            kernel = kernel.transpose([2,3,1,0])\n",
    "            conv_layer.set_weights([kernel])\n",
    "        \n",
    "    print('Model loaded with pre-trained weights.')\n",
    "else:\n",
    "    print('Weights file doesn\\'t exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists('CheckPoint/weights.10-1.57.hdf5')):\n",
    "    from keras.models import load_model\n",
    "    model.load_weights('CheckPoint/weights.10-1.57.hdf5')\n",
    "    print('Model loaded with pre-trained weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DOWNLOAD_DATA == True):\n",
    "    # Save current directory path\n",
    "    curr_dir = os.getcwd()\n",
    "\n",
    "    # Training Data\n",
    "    if(os.path.isdir(\"data\") and os.path.exists(\"data/train2014.zip\")):\n",
    "        if(os.path.isdir(\"data/Ext/images/train2014\") and len(os.listdir('data/Ext/images/train2014')) != 0):\n",
    "            print('Training files available on disk.')\n",
    "        else:\n",
    "            with zipfile.ZipFile(\"data/train2014.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"data/Ext/images/\")\n",
    "    else:\n",
    "        if(os.path.isdir(\"data\")):\n",
    "            os.chdir(\"data/\")\n",
    "        else:\n",
    "            os.makedirs(\"data/Ext/images\")\n",
    "            os.chdir(\"data/\")\n",
    "        url = 'http://images.cocodataset.org/zips/train2014.zip'\n",
    "        filename = wget.download(url)\n",
    "        with zipfile.ZipFile(\"train2014.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./Ext/images\")\n",
    "\n",
    "    os.chdir(curr_dir)\n",
    "\n",
    "    # Training Data Annotation\n",
    "    if(os.path.isdir(\"data\") and os.path.exists(\"data/annotations_trainval2014.zip\")):\n",
    "        if(os.path.isdir(\"data/Ext/annotations\") and len(os.listdir('data/Ext/annotations')) != 0):\n",
    "            print('Training annotation files available on disk.')\n",
    "        else:\n",
    "            with zipfile.ZipFile(\"data/annotations_trainval2014.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"data/Ext/\")\n",
    "    else:\n",
    "        if(os.path.isdir(\"data\")):\n",
    "            os.chdir(\"data/\")\n",
    "        else:\n",
    "            os.makedirs(\"data\")\n",
    "            os.chdir(\"data/\")\n",
    "        url = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n",
    "        filename = wget.download(url)\n",
    "        with zipfile.ZipFile(\"annotations_trainval2014.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./Ext/\")\n",
    "\n",
    "    os.chdir(curr_dir)\n",
    "\n",
    "    # Validation/Testing Data Annotation\n",
    "    if(os.path.isdir(\"data\") and os.path.exists(\"data/val2014.zip\")):\n",
    "        if(os.path.isdir(\"data/Ext/images/val2014\") and len(os.listdir('data/Ext/images/val2014')) != 0):\n",
    "            print('Validation / Testing files available on disk.')\n",
    "        else:\n",
    "            with zipfile.ZipFile(\"data/val2014.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"data/Ext/images/\")\n",
    "    else:\n",
    "        if(os.path.isdir(\"data\")):\n",
    "            os.chdir(\"data/\")\n",
    "        else:\n",
    "            os.makedirs(\"data\")\n",
    "            os.chdir(\"data/\")\n",
    "        url = 'http://images.cocodataset.org/zips/val2014.zip'\n",
    "        filename = wget.download(url)\n",
    "        with zipfile.ZipFile(\"val2014.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./Ext/images/\")\n",
    "\n",
    "    os.chdir(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data from COCO format to VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DOWNLOAD_DATA == True):\n",
    "    os.makedirs(\"data/Ext/images/train2014ann\")\n",
    "    !python coco2pascal.py create_annotations data/Ext train data/Ext/images/train2014ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DOWNLOAD_DATA == True):\n",
    "    os.makedirs(\"data/Ext/images/val2014ann\")\n",
    "    !python coco2pascal.py create_annotations data/Ext val data/Ext/images/val2014ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data   = \"data/Ext/images/train2014/\"\n",
    "training_data_ann = \"data/Ext/images/train2014ann/\"\n",
    "validation_data = \"data/Ext/images/val2014/\"\n",
    "validation_data_ann = \"data/Ext/images/val2014ann/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse annotations to generator training and validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "    'IMAGE_H'          : IMAGE_H,\n",
    "    'IMAGE_W'          : IMAGE_W,\n",
    "    'GRID_H'           : GRID_H,\n",
    "    'GRID_W'           : GRID_W,\n",
    "    'BOX'              : BOX,\n",
    "    'LABELS'           : LABELS,\n",
    "    'CLASS'            : len(LABELS),\n",
    "    'ANCHORS'          : ANCHORS,\n",
    "    'BATCH_SIZE'       : (BATCH_SIZE * 10),\n",
    "    'TRUE_BOX_BUFFER'  : TRUE_BOX_BUFFER,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return image / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, seen_train_labels = parse_annotation(training_data_ann, \\\n",
    "                                                 training_data,     \\\n",
    "                                                 labels=LABELS)\n",
    "\n",
    "train_batch = BatchGenerator(train_imgs, gen_config, norm=normalize)\n",
    "\n",
    "\n",
    "valid_imgs, seen_valid_labels = parse_annotation(validation_data_ann, \\\n",
    "                                                 validation_data,     \\\n",
    "                                                 labels=LABELS)\n",
    "\n",
    "valid_batch = BatchGenerator(valid_imgs, gen_config, norm=normalize, jitter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3d3a6374539d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseen_train_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_annotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_ann\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m                                                      \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "train_imgs, seen_train_labels = parse_annotation(training_data_ann, \\\n",
    "                                                 training_data,     \\\n",
    "                                                 labels=LABELS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = BatchGenerator(train_imgs, gen_config, norm=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 0\n",
    "for [X1, X2], Y in train_batch:\n",
    "    np.savez('data/Ext/images/train2014npz/np-{0:0=5d}.npz'.format(train), X1=X1, X2=X2, Y=Y)\n",
    "    train += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = 0\n",
    "for [X1, X2], Y in valid_batch:\n",
    "    np.savez('data/Ext/images/valid2014npz/np-{0:0=5d}.npz'.format(train), X1=X1, Y=Y)\n",
    "    train += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 480 480\n",
      "Epoch 1/100\n",
      " - 7s - loss: 1.8508\n",
      "Epoch 2/100\n",
      " - 7s - loss: 6.2940\n",
      "Epoch 3/100\n",
      " - 7s - loss: 2.1547\n",
      "Epoch 4/100\n",
      " - 7s - loss: 1.9563\n",
      "Epoch 5/100\n",
      " - 7s - loss: 1.7377\n",
      "Epoch 6/100\n",
      " - 7s - loss: 1.5889\n",
      "Epoch 7/100\n",
      " - 7s - loss: 1.4197\n",
      "Epoch 8/100\n",
      " - 7s - loss: 1.3412\n",
      "Epoch 9/100\n",
      " - 7s - loss: 1.2225\n",
      "Epoch 10/100\n",
      " - 7s - loss: 1.1859\n",
      "Epoch 11/100\n",
      " - 7s - loss: 1.1730\n",
      "Epoch 12/100\n",
      " - 7s - loss: 1.1543\n",
      "Epoch 13/100\n",
      " - 7s - loss: 1.0806\n",
      "Epoch 14/100\n",
      " - 7s - loss: 1.1327\n",
      "Epoch 15/100\n",
      " - 7s - loss: 1.0688\n",
      "Epoch 16/100\n",
      " - 7s - loss: 1.0666\n",
      "Epoch 17/100\n",
      " - 7s - loss: 0.9984\n",
      "Epoch 18/100\n",
      " - 7s - loss: 0.9835\n",
      "Epoch 19/100\n",
      " - 7s - loss: 1.0029\n",
      "Epoch 20/100\n",
      " - 7s - loss: 0.9575\n",
      "Epoch 21/100\n",
      " - 7s - loss: 0.9149\n",
      "Epoch 22/100\n",
      " - 7s - loss: 1.0353\n",
      "Epoch 23/100\n",
      " - 7s - loss: 0.9691\n",
      "Epoch 24/100\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    Y  = []\n",
    "    file = 'data/Ext/images/train2014npz/np-{0:0=5d}.npz'.format(i)\n",
    "    data = np.load(file)\n",
    "    X1.append(data['X1'])\n",
    "    X2.append(data['X2'])\n",
    "    Y.append(data['Y'])\n",
    "    print(len(X1[0]), len(X2[0]), len(Y[0]))\n",
    "    model.fit([X1[0], X2[0]], Y[0], batch_size=48, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 1 - Classification Loss\n",
    "\n",
    "if an object is detected, the classification loss at each cell is the squared error of the class conditional probabilities for each loss:\n",
    "\n",
    "<img src=\"images/ClassificationLoss.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u> **Figure 1** </u>: **Classification Loss**<br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_loss(y_true, y_pred):\n",
    "\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    # true classification box\n",
    "    true_box_class = tf.argmax(y_true[...,5:], -1)\n",
    "    \n",
    "    # classification mask\n",
    "    class_mask = y_true[...,4] * tf.gather(CLASS_WEIGHTS, true_box_class)\n",
    "\n",
    "    # number of classification boxes\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    # predicted box\n",
    "    pred_box_class = y_pred[...,5:]\n",
    "\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    \n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + EPSILON)\n",
    "    \n",
    "    loss_class = tf.Print(loss_class, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    \n",
    "    return loss_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Localization Loss\n",
    "\n",
    "The localization loss measures the errors in the predicted boundary box locations and sizes. We only count the box responsible for detecting the object.\n",
    "\n",
    "<img src=\"images/LocalizationLoss.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **Localization Loss**<br> </center></caption>\n",
    "\n",
    "We do not want to weight absolute errors in large boxes and small boxes equally. i.e. a 2-pixel error in a large box is the same for a small box. To partially address this, YOLO predicts the square root of the bounding box width and height instead of the width and height. In addition, to put more emphasis on the boundary box accuracy, we multiply the loss by λcoord (default: 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "    \n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically   \n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1)\n",
    "    \n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    \n",
    "    Localization_Loss = loss_xy + loss_wh\n",
    "    \n",
    "    # Localization_Loss = tf.Print(Localization_Loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    # Localization_Loss = tf.Print(Localization_Loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    \n",
    "    return Localization_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Confidence Loss\n",
    "\n",
    "If an object is detected in the box, the confidence loss (measuring the objectness of the box) is:\n",
    "\n",
    "<img src=\"images/ConfidenceLoss1.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u> **Figure 3** </u>: **Confidence Loss**<br> </center></caption>\n",
    "\n",
    "If an object is not detected in the box, the confidence loss is:\n",
    "\n",
    "<img src=\"images/ConfidenceLoss2.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u> **Figure 4** </u>: **Confidence Loss**<br> </center></caption>\n",
    "\n",
    "Most boxes do not contain any objects. This causes a class imbalance problem, i.e. we train the model to detect background more frequently than detecting objects. To remedy this, we weight this loss down by a factor λnoobj (default: 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_loss(y_true, y_pred):\n",
    "    \n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "    \n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    \n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    \n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "    \n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4])\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf - pred_box_conf) * conf_mask)  / (nb_conf_box  + EPSILON) / 2.\n",
    "    \n",
    "    # loss_conf = tf.Print(loss_conf, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    \n",
    "    return loss_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss\n",
    "\n",
    "The final loss adds localization, confidence and classification losses together.\n",
    "\n",
    "<img src=\"images/TotalLoss.png\" style=\"width:500px;height:250;\">\n",
    "<caption><center> <u> **Figure 5** </u>: **Total Loss**<br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(y_true, y_pred):\n",
    "    loss = classification_loss(y_true, y_pred) + localization_loss(y_true, y_pred) + confidence_loss(y_true, y_pred)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://keras.io/optimizers/\n",
    "\n",
    "Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "'''\n",
    "\n",
    "optimizer = Adam(lr=0.5e-4, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://keras.io/models/sequential/\n",
    "\n",
    "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, \\\n",
    "                                                        weighted_metrics=None, target_tensors=None)\n",
    "'''\n",
    "model.compile(loss=total_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check point directory exists.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://keras.io/callbacks/\n",
    "\n",
    "EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', \\\n",
    "                                                  baseline=None, restore_best_weights=False)\n",
    "\n",
    "ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False,  \\\n",
    "                                              save_weights_only=False, mode='auto', period=1)\n",
    "'''\n",
    "EarlyStop = EarlyStopping(min_delta=0.001, patience=5, mode='min', verbose=1)\n",
    "\n",
    "if(os.path.isdir(\"CheckPoint\")):\n",
    "    print('Check point directory exists.')\n",
    "else:\n",
    "    os.makedirs(\"CheckPoint\")\n",
    "\n",
    "MakeCheckPoint = ModelCheckpoint(\"CheckPoint/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", \\\n",
    "                                 verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 131/1425 [=>............................] - ETA: 40:12 - loss: 1.9482"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://keras.io/models/sequential/\n",
    "\n",
    "fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, \\\n",
    "                   validation_data=None, validation_steps=None, class_weight=None,  \\\n",
    "                   max_queue_size=10, workers=1, use_multiprocessing=False,         \\\n",
    "                   shuffle=True, initial_epoch=0)\n",
    "'''\n",
    "\n",
    "model.fit_generator(\n",
    "                    generator        = train_batch,\n",
    "                    steps_per_epoch  = len(train_batch),\n",
    "                    epochs           = 10,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = [EarlyStop, MakeCheckPoint],\n",
    "                    max_queue_size   = 10\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-max suppression ###\n",
    "\n",
    "Even after filtering by thresholding over the classes scores, you still end up a lot of overlapping boxes. A second filter for selecting the right boxes is called non-maximum suppression (NMS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/non-max-suppression.png\" style=\"width:500px;height:400;\">\n",
    "<caption><center> <u> **Figure 5** </u>: In this example, the model has predicted 3 cars, but it's actually 3 predictions of the same car. Running non-max suppression (NMS) will select only the most accurate (highest probabiliy) one of the 3 boxes. <br> </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-max suppression uses the very important function called **\"Intersection over Union\"**, or IoU.\n",
    "<img src=\"images/iou.png\" style=\"width:500px;height:400;\">\n",
    "<caption><center> <u> **Figure 6** </u>: Definition of \"Intersection over Union\". <br> </center></caption> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tiny YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(xmodel, image_file):\n",
    "\n",
    "    # Preprocess your image\n",
    "    image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (IMAGE_H, IMAGE_W))\n",
    "\n",
    "\n",
    "    _array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "    netout = xmodel.predict([image_data, _array])\n",
    "\n",
    "    return netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_network_output(netout):\n",
    "    return decode_netout(netout[0], \n",
    "                        anchors=ANCHORS, \n",
    "                        nb_class=CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell on the \"test.jpg\" image to verify that your function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netout = predict(model, \"COCO_val2014_000000385918.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = decode_network_output(netout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in boxes:\n",
    "    print(LABELS[box.get_label()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener's github repository. The pretrained weights used in this exercise came from the official YOLO website. \n",
    "- Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (2015)\n",
    "- Joseph Redmon, Ali Farhadi - [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) (2016)\n",
    "- Allan Zelener - [YAD2K: Yet Another Darknet 2 Keras](https://github.com/allanzelener/YAD2K)\n",
    "- The official YOLO website (https://pjreddie.com/darknet/yolo/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batch = BatchGenerator(train_imgs, gen_config, norm=normalize)\n",
    "\n",
    "\n",
    "\n",
    "valid_batch = BatchGenerator(valid_imgs, gen_config, norm=normalize, jitter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OMdut",
   "launcher_item_id": "bbBOL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
