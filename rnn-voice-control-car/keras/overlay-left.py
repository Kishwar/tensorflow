from pydub import AudioSegment
from configparser import ConfigParser
import os
import numpy as np
import random
import sys
import IPython
from scipy.io import wavfile 
import matplotlib
import matplotlib.pyplot as plt

'''
# Data downloaded from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz

Copyright note:

APA-style citation: "Warden P. Speech Commands: A public dataset for single-word speech recognition, 2017. Available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz".

BibTeX @article{speechcommands, title={Speech Commands: A public dataset for single-word speech recognition.}, author={Warden, Pete}, journal={Dataset available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz}, year={2017} }
'''

fileIdx = 0

# instantiate
config = ConfigParser()

# parse existing file
config.read('config.ini')

root = config.get('section_path', 'data_path')
data = config.get('section_path', 'training_data')

# path to noise data - 3 seconds each generated by 'gen_noise_data.py'
noise = data + config.get('section_path', 'out_noise_path')

inPath = root + config.get('section_path', 'left')
outPath = data + config.get('section_path', 'left')

print(inPath)
print(outPath)

Ty = 1375


# From Audio recordings to Spectrograms
def graph_spectrogram(wav_file):
    rate, data = wavfile.read(wav_file)  # rate=44100 - sampling rate (https://en.wikipedia.org/wiki/44,100_Hz)
    
    nfft = 200          # fft window length
    fs = 8000           # sampling frequency
    noverlap = 120      # overlap between windows
    nchannels = data.ndim
    
    if nchannels == 1:
        pxx, freq, bins, im = plt.specgram(data, nfft, fs, noverlap=noverlap)
    elif nchannels == 2:
        pxx, freq, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap=noverlap)
    
    # print('file duration in seconds %s' %(len(data)/rate))
    return pxx    
    
# find random time segment
def get_random_time_segment(seg_ms):
    seg_start = np.random.randint(low=0, high=3000-seg_ms)    # low = 0 sec, high = 3 seconds
    seg_end = seg_start + seg_ms - 1
    #print('end' + str(seg_start))
    return seg_start, seg_end

    
# method to overlay
def insert_audio_clip(background, audio_clip):
    """
    background     10 sec background noise audio
    audio_clip     clip with positive or negative sample
    prev_seg       time where autdio sample has already placed
    
    return         updated 10 seconds audio
    """
    
    # get time in ms
    seg_ms = len(audio_clip)
    
    # get random place time for this clip
    seg_time = get_random_time_segment(seg_ms)
    
    print(seg_time)
    
    # lets overlay
    new_background = background.overlay(audio_clip, position = seg_time[0])
    
    return new_background, seg_time
    
# generate Y by inserting 1 at place where keyword 'left' is detected
def insert_ones(y, seg_end_ms):
    
    seg_end_y = int(seg_end_ms * Ty / 3000.0)
    
    for i in range(seg_end_y + 1, seg_end_y + 51): # 50 1s once keyword detected
        if i < Ty:
            y[0, i] = 1
            
    return y

# amplify it
def match_target_amp(sound, target_dBFS):
    dBFS_Change = target_dBFS - sound.dBFS
    return sound.apply_gain(dBFS_Change)

# create training samples 
def create_training_sample(background, positive, idx):
    
    """
    background      path 3 second audio
    positive        path positive sample audio  -> with left
    
    returns:
    x               spectrogram of training sample
    y               label at each time step of the spectrogram
    """
    
    # Make backgound quieter
    background = background - 20
    
    # initialize ouput vector as zeros
    y = np.zeros((1, Ty))
    
    # insert positive sample on background
    background, seg_time = insert_audio_clip(background, positive[0])
        
    # retrieve seg_start and seg_end from seg_time
    seg_start, seg_end = seg_time
        
    # insert ones in y
    y = insert_ones(y, seg_end)  
    
    # standarize the volume of the audio clip
    backgound = match_target_amp(background, -20.0)
    
    # file name
    name = outPath + str(idx) + ".wav"
    
    # save on disk
    background = backgound.export(name, format="wav")
    
    # get spectrogram
    x = graph_spectrogram(name)
        
    return x, y

# ----------------------------------------------
# LETS GENERATE KEY WORD 'LEFT' SAMPLE WAV FILES
# ----------------------------------------------
if __name__ == "__main__":
    background = []
    positive = []
    
    X = []
    Y = []
    
    # read all noise files in an array
    for file in os.listdir(noise):
        if file.endswith("wav"):
            bak = AudioSegment.from_wav(noise + file)
            background.append(bak)
    
    fileIdx = 0
    npIndx = 0
    for i in range(len(background)):		
    	count = 0       
    	for file in os.listdir(inPath):
    		if file.endswith("wav"):
    			count += 1
    	for file in os.listdir(inPath):
            if file.endswith("wav"):
                if(count % 5 == 0):
                    LeftX, LeftY = create_training_sample(background[i], positive, fileIdx)
                    positive = []
                    fileIdx += 1
                    X.append(LeftX)
                    Y.append(LeftY)
                    if(count % 2000 == 0):
                        X = np.asarray(X)
                        Y = np.asarray(Y)
                        np.save(outPath + "np-array\X-" + str(npIndx) + "-.npy", X)
                        np.save(outPath + "np-array\Y-" + str(npIndx) + "-.npy", Y)
                        X = []
                        Y = []
                        npIndx += 1
                pos = AudioSegment.from_wav(inPath + file)
                positive.append(pos)
                count -= 1
    X = []
    Y = []